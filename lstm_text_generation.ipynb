{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgTMNQr677kz"
   },
   "source": [
    "# LSTM text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhttYUNo77k1"
   },
   "source": [
    "Example script to generate text from text files.\n",
    "\n",
    "At least 20 epochs are required before the generated text starts sounding coherent.\n",
    "\n",
    "It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive.\n",
    "\n",
    "If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifH621qw77k2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15337693887161334341\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17954772553750801969\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2842435971729058302\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5916393472\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3123601243359866979\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import concatenate\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RE5YewP77k9"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, text, char_indices, batch_size=128, maxlen=40, step=3):\n",
    "        self.text = text\n",
    "        self.char_indices = char_indices\n",
    "        self.batch_size = batch_size\n",
    "        self.maxlen = maxlen\n",
    "        self.step = step\n",
    "\n",
    "    def __len__(self):\n",
    "        return ((len(self.text) - self.maxlen) // self.step) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = np.zeros((self.batch_size, self.maxlen, len(self.char_indices)), dtype=np.bool)\n",
    "        y = np.zeros((self.batch_size, len(self.char_indices)), dtype=np.bool)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            idx = (i + index) * self.step\n",
    "\n",
    "            for t, char in enumerate(self.text[idx: idx + self.maxlen]):\n",
    "                x[i, t, self.char_indices[char]] = 1\n",
    "\n",
    "            y[i, self.char_indices[self.text[idx + self.maxlen]]] = 1\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "LPPBV02j77lD",
    "outputId": "d703240b-1ef1-42ea-becf-0ae184cb9f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 13137488\n",
      "total chars: 123\n"
     ]
    }
   ],
   "source": [
    "path = get_file(\n",
    "    'lovecraft.txt',\n",
    "    origin='https://bashkirtsevich.pro/shared/lovecraft.txt'\n",
    ")\n",
    "\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjnHLEAQ77lI"
   },
   "outputs": [],
   "source": [
    "maxlen = 40  # cut the text in semi-redundant sequences of maxlen characters\n",
    "training_generator = DataGenerator(text, char_indices, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBY-wKj-77lO"
   },
   "source": [
    "## Build the model: a single LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9o2W_0qw77lR"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print(f'----- Generating text after Epoch: {epoch + 1}')\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(f'----- diversity: {diversity}')\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print(f'----- Generating with seed: \"{sentence}\"')\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "        print(generated)\n",
    "        \n",
    "    model.save(f\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "MotBNxq877lU",
    "outputId": "759d6659-d023-48ac-dbfb-a13866babdba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0623 23:09:22.252649 140036356044608 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0623 23:09:22.255193 140036356044608 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0623 23:09:22.262649 140036356044608 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 23:09:22.619403 140036356044608 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0623 23:09:22.626896 140036356044608 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0623 23:09:23.500063 140036356044608 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0623 23:09:23.509659 140036356044608 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build/load the model: a single LSTM\n",
    "model_path = None\n",
    "\n",
    "if model_path:\n",
    "    print('Load model...')\n",
    "    model = load_model(model_path)\n",
    "else:\n",
    "    print('Build model...')\n",
    "\n",
    "    num_chars = len(chars)\n",
    "\n",
    "    vec = Input(shape=(maxlen, num_chars))\n",
    "    l1 = LSTM(activation='tanh', return_sequences=True, units=128)(vec)\n",
    "    l1_d = Dropout(0.2)(l1)\n",
    "\n",
    "    input2 = concatenate([vec, l1_d])\n",
    "    l2 = LSTM(activation='tanh', return_sequences=True, units=128)(input2)\n",
    "    l2_d = Dropout(0.2)(l2)\n",
    "\n",
    "    input3 = concatenate([vec, l2_d])\n",
    "    l3 = LSTM(activation='tanh', return_sequences=True, units=128)(input3)\n",
    "    l3_d = Dropout(0.2)(l3)\n",
    "\n",
    "    input_d = concatenate([l1_d, l2_d, l3_d])\n",
    "    \n",
    "    l4 = LSTM(activation='tanh', return_sequences=False, units=128)(input_d)\n",
    "    l4_d = Dropout(0.2)(l4)\n",
    "    \n",
    "    dense3 = Dense(units=num_chars)(l4_d)\n",
    "    output_res = Activation('softmax')(dense3)\n",
    "    \n",
    "    model = Model(inputs=vec, outputs=output_res)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.), metrics=['accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "8egrG6STg2NA",
    "outputId": "e4dcba5d-47cc-4c31-d596-8633f838aab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 123)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 40, 128)      129024      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40, 128)      0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 40, 251)      0           input_1[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 40, 128)      194560      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 40, 128)      0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 40, 251)      0           input_1[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 40, 128)      194560      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 40, 128)      0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 40, 384)      0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 128)          262656      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 123)          15867       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 123)          0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 796,667\n",
      "Trainable params: 796,667\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "BPZY4N2D77la",
    "outputId": "b46f716d-7bf4-4e56-f673-ea6e72c2811b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 23:09:23.867537 140036356044608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " 7218/34212 [=====>........................] - ETA: 2:27:46 - loss: 2.3440 - acc: 0.3064"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=training_generator,\n",
    "    epochs=60,\n",
    "    callbacks=[\n",
    "        LambdaCallback(on_epoch_end=on_epoch_end),\n",
    "        EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=3, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_text_generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
